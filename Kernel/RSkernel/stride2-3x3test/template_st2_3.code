#include <stdio.h>
#include <stdlib.h>
#include "common.h"

///Only support crsf format kernel
// TODO: ASSUME Input should be padded already
// x=w y=h z=k
__global__ void
conv_kernel(const TYPE *__restrict__ Input /*(C*H*W)*/,
            const TYPE *__restrict__ Kernel /*(C*R*S*F)*/,
            TYPE *__restrict__ Output /*(F*C*Y*X)*/) {

    /* This is an attempt to build on register optimization with shared memory with an alternate view of block orientation with a split c on global 
    that also performs the operation with stride 2. */

    // Calc'ed values and vars
    int k, h, w, c, i, j, r, s, row, kReg, hReg, wReg, kGlobal;
   // Total sizes of input for register operations
    int inTotalH = 2*H+R-2; // TODO: This works but does not seem right
    int inTotalW = 2*W+S-2;


    // Compinsate for c block increase
    int blockIdxXAdjusted = blockIdx.x % BlockXCoverageNumber;

    // Find the upper corner in 3d space
    int kIndex = blockIdxXAdjusted * blockDim.x * T_k + threadIdx.x;
    //int wIndex = (blockIdx.y * blockDim.y + threadIdx.y) * T_w;
    //int hIndex = (blockIdx.z * blockDim.z + threadIdx.z) * T_h;
    int kWarpReach = 32 * T_k; // TODO: Should this be 32 exactly? - Yes?

    //int cRange = C / T_c; // TODO: Assume this will work since T_c is currently 1, but may not always be so
    //int inAdjustedTileH = T_h + R - 1; // Made into constants
    //int inAdjustedTileW = T_w + S - 1;

    //C offset calculation
    int cStart = (blockIdx.x / BlockXCoverageNumber) * CDivisionSize;
    int cBound = cStart + CDivisionSize;

    // Registers
    TYPE regOut[T_k][T_h][T_w]; //4x4x5
    TYPE regIn[inAdjustedTileH][inAdjustedTileW]; //6x7
    TYPE oneKern;

    // Zero out the registers
    for(k = 0; k < T_k; k++) {
        for (h = 0; h < T_h; h++) {
            for (w = 0; w < T_w; w++) {
                regOut[k][h][w] = 0.0f;
            }
        }
    }

    // Shared memory
    //int memTileH = T_h * blockDim.y + R - 1; // Made into constants T_h * B_h + R - 1
    //int memTileW = T_w * blockDim.x + S - 1; // T_w * B_w + S - 1
    int adjustedRowIndexMemTile = threadIdx.z * blockDim.y + threadIdx.y;
    int jumpDist = blockDim.z * blockDim.y;
    int hIndexBlock = blockIdx.z * blockDim.z * T_h;
    int wIndexBlock = blockIdx.y * blockDim.y * T_w;

    __shared__ TYPE shareBuffer_In[memTileH][memTileW]; // TODO: make non-square to avoid conflicts - if needed
    __shared__ TYPE shareBuffer_Out[OutBlockH][OutBlockW][OutBlockK+1];

    // Collapse all c into output
    for (c = cStart; c < cBound; c++) { // Assuming T_c is 1
        // Fill in shared memory - together
        for(row = adjustedRowIndexMemTile; row < memTileH; row += jumpDist) {
            if(threadIdx.x < memTileW) {
                shareBuffer_In[row][threadIdx.x] = Input[ (c)*inTotalH*inTotalW + (2 * hIndexBlock + row)*inTotalW + (2 * wIndexBlock + threadIdx.x) ];
            }
        }

        __syncthreads();

        // Fill regesters In - just ours
        for(i = 0; i < inAdjustedTileH; i++) {
            for(j = 0; j < inAdjustedTileW; j++) {
                regIn[i][j] = shareBuffer_In[threadIdx.z*2*T_h + i][threadIdx.y*2*T_w + j];
            }
        }
        
        // Use the input in regesters for all k in the tile
        kReg = 0;
#pragma unroll(4) // ReplaceLine 000001
        for (kGlobal = 0; kGlobal < kWarpReach; kGlobal+=32) { // 4
            // Do actual calculations
#pragma unroll(1) // ReplaceLine 000002
            for (r = 0; r < R; r++) {
#pragma unroll(1) // ReplaceLine 000003
                for (s = 0; s < S; s++) {
                    oneKern = Kernel[ (c)*R*S*K + (r)*S*K + (s)*K + (kIndex + kGlobal) ];
                    hReg = 0;
#pragma unroll(2) // ReplaceLine 000004
                    for (h = r; h < (T_h)*2 + r - 1; h+=2) {
                        wReg = 0;
#pragma unroll(2) // ReplaceLine 000005
                        for (w = s; w < (T_w)*2 + s - 1; w+=2) {
                            regOut[kReg][hReg][wReg] += regIn[h][w] * oneKern;
                            wReg += 1;
                        }
                        hReg += 1;
                    }
                }
            }
            kReg++; // RegisterK
        } // End k

        __syncthreads(); // Sync for next loop

    } // End c

    int kIndexTBlock = threadIdx.x;
    int hIndexTBlock = threadIdx.z * T_h;
    int wIndexTBlock = threadIdx.y * T_w;
    kWarpReach = 32;
    int kIndexBlock;

    for(k = 0; k < T_k; k++) {

        // Fill Out from Reg to shared mem
        for (h = 0; h < T_h; h++) {
            for (w = 0; w < T_w; w++) {
                shareBuffer_Out[hIndexTBlock + h][wIndexTBlock + w][kIndexTBlock] = regOut[k][h][w];
            }
        }

        __syncthreads();

        kIndexBlock = blockIdxXAdjusted * blockDim.x * T_k + k * 32;

        // Fill Out from shared memory
        for(kGlobal = 0; kGlobal < OutBlockK; kGlobal++) {
            for(row = adjustedRowIndexMemTile; row < OutBlockH; row += jumpDist) {
                if(threadIdx.x < OutBlockW) {
                    atomicAdd(&Output[ (kIndexBlock + kGlobal)*H*W + (hIndexBlock + row)*W + (wIndexBlock + threadIdx.x) ], shareBuffer_Out[row][threadIdx.x][kGlobal]);
                }
            }
        }

        __syncthreads();
    }

} // END

